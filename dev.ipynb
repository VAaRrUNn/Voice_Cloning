{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanat\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torchaudio\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import lib\n",
    "import librosa\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 319]), torch.Size([1, 64, 319]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    \"Dataset for loading/preprocessing of audio data\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 audio_dir: os.path or str,\n",
    "                 transformation: torchaudio.transforms,\n",
    "                 sample_rate: int,\n",
    "                 num_samples: int,\n",
    "                 device: torch.device):\n",
    "        \"\"\"\n",
    "        Initializes the constructor\n",
    "        Parameters: audio_dir: path to the audio directory\n",
    "        transformation: transformation applied to the audio\n",
    "        sample_rate: sampling rate.\n",
    "        \"\"\"\n",
    "\n",
    "        super(MusicDataset, self).__init__()\n",
    "        self.audio_dir = audio_dir\n",
    "        self.transformation = transformation.to(device)\n",
    "        self.target_sample_rate = sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return length of dataset (audio samples)\n",
    "        \"\"\"\n",
    "        return len(os.listdir(self.audio_dir))\n",
    "\n",
    "    def __getitem__(self, index) -> torch.Tensor:\n",
    "        audio_sample_path = os.path.join(\n",
    "            self.audio_dir, os.listdir(self.audio_dir)[index])\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "\n",
    "        signal = self.transformation(signal)\n",
    "        return signal\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        \"\"\"\n",
    "        Trims the part of signal if signal length > num_samples\n",
    "        \"\"\"\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        \"\"\"\n",
    "        Pads the signal if signal length < num_samples\n",
    "        \"\"\"\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            singal = F.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        \"\"\"\n",
    "        Resamples the audio if needed sample rate and the audio sample rate are not same\n",
    "        \"\"\"\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(\n",
    "                sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "\n",
    "Audio_dir = os.path.join(os.getcwd(), \"audio_files\")\n",
    "SAMPLE_RATE = 22050\n",
    "NUM_SAMPLES = 163180\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "mel_spectogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=64\n",
    ")\n",
    "\n",
    "music_dataset = MusicDataset(audio_dir=Audio_dir,\n",
    "                             transformation=mel_spectogram,\n",
    "                             sample_rate=SAMPLE_RATE,\n",
    "                             num_samples=NUM_SAMPLES,\n",
    "                             device=device)\n",
    "\n",
    "music_dataset[0].shape, music_dataset[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = \"abcd efghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_-,.?;':!@#$%^&*()\"\n",
    "char_to_idx = {k: v for v, k in enumerate(characters)}\n",
    "idx_to_char = {k: v for k, v in enumerate(characters)}\n",
    "\n",
    "PADDING_TOKEN = '?'\n",
    "MAX_SEQ_LEN = 1276\n",
    "N_EMBD = 256\n",
    "vocab_size = len(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_tensor(char,\n",
    "                   vocab_to_index,\n",
    "                   vocab_size):\n",
    "    # print(char)\n",
    "    char_tensor = torch.zeros((vocab_size))\n",
    "    char_tensor[vocab_to_index[char]] = 1\n",
    "    return char_tensor\n",
    "\n",
    "\n",
    "def sentence_to_tensor(sen,\n",
    "                       vocab_to_index,\n",
    "                       vocab_size,\n",
    "                       max_seq_len):\n",
    "    # print(sen)\n",
    "    padding_tensor = char_to_tensor(char=PADDING_TOKEN,\n",
    "                                    vocab_to_index=vocab_to_index,\n",
    "                                    vocab_size=vocab_size)\n",
    "    sen_tensor = torch.stack([padding_tensor] * max_seq_len)\n",
    "    sen = sen[:max_seq_len-1]\n",
    "    # print(sen_tensor.shape)\n",
    "    for i, char in enumerate(sen):\n",
    "        char_tensor = char_to_tensor(char=char,\n",
    "                                     vocab_to_index=vocab_to_index,\n",
    "                                     vocab_size=vocab_size)\n",
    "        sen_tensor[i] = char_tensor\n",
    "    return sen_tensor\n",
    "\n",
    "\n",
    "def sentences_to_tensor(sentences,\n",
    "                        vocab_to_index,\n",
    "                        vocab_size,\n",
    "                        max_seq_len):\n",
    "    batch_size = len(sentences)\n",
    "    sens_tensor = torch.zeros(batch_size, max_seq_len, vocab_size)\n",
    "    for i, sen in enumerate(sentences):\n",
    "        sen_tensor = sentence_to_tensor(sen=sen,\n",
    "                                        vocab_to_index=vocab_to_index,\n",
    "                                        vocab_size=vocab_size,\n",
    "                                        max_seq_len=max_seq_len)\n",
    "        sens_tensor[i] = sen_tensor\n",
    "    return sens_tensor\n",
    "\n",
    "\n",
    "# sentence_to_tensor(sen=\"my\",\n",
    "#                    vocab_to_index=char_to_idx,\n",
    "#                    vocab_size=vocab_size,\n",
    "#                    max_seq_len=MAX_SEQ_LEN).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi torch.Size([1276, 71])\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_dir, device):\n",
    "        super(TextDataset, self).__init__()\n",
    "\n",
    "        self.text_dir = text_dir\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.text_dir))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text_file = os.path.join(\n",
    "            self.text_dir, os.listdir(self.text_dir)[index])\n",
    "        with open(text_file, 'r') as f:\n",
    "            text = f.read().replace('\\n', '').replace('â', \"\").replace(\n",
    "                \"€\", \"\").replace(\"™\", \"\").replace(\"…\", \"\")\n",
    "\n",
    "        return sentence_to_tensor(sen=text,\n",
    "                                  vocab_to_index=char_to_idx,\n",
    "                                  vocab_size=vocab_size,\n",
    "                                  max_seq_len=MAX_SEQ_LEN)\n",
    "\n",
    "\n",
    "Text_dir = os.path.join(os.getcwd(), \"text_files\")\n",
    "text_dataset = TextDataset(text_dir=Text_dir,\n",
    "                           device=device)\n",
    "print(f\"hi {text_dataset[5].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "\n",
    "def create_dataloader(train_data, batch_size):\n",
    "    train_dataloder = DataLoader(train_data, batch_size=batch_size)\n",
    "    return train_dataloder\n",
    "\n",
    "\n",
    "text_dataloader = create_dataloader(train_data=text_dataset,\n",
    "                                    batch_size=BATCH_SIZE)\n",
    "audio_dataloader = create_dataloader(train_data=music_dataset,\n",
    "                                     batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "    def forward(self,\n",
    "                input_encodings):\n",
    "\n",
    "        batch_size, max_seq_len, n_embd = input_encodings.size()\n",
    "        i = torch.arange(0, n_embd, 2).float()\n",
    "\n",
    "        denominator = torch.pow(10_000, (i)/n_embd).type(torch.float32)\n",
    "        position = torch.arange(\n",
    "            0, max_seq_len, dtype=torch.float32).reshape(max_seq_len, 1)\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2).view(max_seq_len, -1)\n",
    "        return stacked.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "\n",
    "class GlobalVarianceNormalization(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super(GlobalVarianceNormalization, self).__init__()\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate the global mean and variance\n",
    "        global_mean = torch.mean(x)\n",
    "        global_var = torch.var(x, unbiased=False) + self.eps\n",
    "\n",
    "        # Normalize the input tensor using the global mean and variance\n",
    "        x = (x - global_mean) / torch.sqrt(global_var)\n",
    "\n",
    "        # Apply learnable scale and shift (weight and bias)\n",
    "        x = x * self.weight + self.bias\n",
    "        return x\n",
    "\n",
    "\n",
    "class FFTBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_embd,\n",
    "                 num_heads,\n",
    "                 device,\n",
    "                 mask,\n",
    "                 kernel_size,\n",
    "                 stride,\n",
    "                 padding,\n",
    "                 max_seq_len):\n",
    "\n",
    "        super(FFTBlock, self).__init__()\n",
    "\n",
    "        self.k = nn.Linear(n_embd, n_embd)\n",
    "        self.q = nn.Linear(n_embd, n_embd)\n",
    "        self.v = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "        self.multiheadAttention = nn.MultiheadAttention(embed_dim=n_embd,\n",
    "                                                        num_heads=num_heads,\n",
    "                                                        dropout=0.3,\n",
    "                                                        device=device)\n",
    "\n",
    "        self.norm = GlobalVarianceNormalization(normalized_shape=n_embd)\n",
    "        self.mask = mask  # check shape\n",
    "\n",
    "        # something for same shape\n",
    "        self.conv1 = nn.Conv1d(in_channels=max_seq_len,\n",
    "                               out_channels=max_seq_len,\n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=stride,\n",
    "                               padding=padding)\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_embd, 2 * n_embd),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2 * n_embd, n_embd),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        k = self.k(x)\n",
    "        q = self.q(x)\n",
    "        v = self.v(x)\n",
    "\n",
    "        temp = self.multiheadAttention(q, k, v, need_weights=False)\n",
    "        # print(temp[0].shape)\n",
    "        temp = temp[0]\n",
    "        temp = self.norm(x + temp)\n",
    "        temp = self.linear(temp)\n",
    "        # print(temp.shape)\n",
    "        x = temp\n",
    "        temp = self.conv1(temp)\n",
    "        temp = self.linear(temp)\n",
    "        # print(temp.shape)\n",
    "        temp = self.norm(temp + x)\n",
    "        # print(f\"returning from fft\")\n",
    "        return temp\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 max_seq_len,\n",
    "                 n_embd,\n",
    "                 device,\n",
    "                 n_heads,\n",
    "                 mask,\n",
    "                 kernel_size,\n",
    "                 padding,\n",
    "                 stride,\n",
    "                 vocab_size):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.text_encoding = nn.Linear(vocab_size, n_embd)\n",
    "        self.positional_encodings = PositionalEncoding()\n",
    "        self.fft = FFTBlock(n_embd=n_embd,\n",
    "                            num_heads=n_heads,\n",
    "                            device=device,\n",
    "                            mask=mask,\n",
    "                            kernel_size=kernel_size,\n",
    "                            stride=stride,\n",
    "                            padding=padding,\n",
    "                            max_seq_len=max_seq_len)\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd // 2),\n",
    "            nn.Linear(n_embd // 2, (319 * 64) // max_seq_len)\n",
    "        )\n",
    "\n",
    "    def forward(self, text_input):\n",
    "        text_input = self.text_encoding(text_input)\n",
    "        # print(f\"After text encoding -> {text_input.shape}\")\n",
    "        pos = self.positional_encodings(text_input)\n",
    "        text_input += pos\n",
    "        text_input = self.fft(text_input)\n",
    "        text_input = self.fft(text_input)\n",
    "        text_input = self.linear(text_input)\n",
    "        return text_input\n",
    "\n",
    "\n",
    "def train_one_step(model, loss_fn, optimizer, text_dataloader, audio_dataloader, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    losses = 0\n",
    "    for text_input, audio_input in tqdm(zip(text_dataloader, audio_dataloader)):\n",
    "\n",
    "        text_input, audio_input = text_input.to(device), audio_input.to(device)\n",
    "        mel_spectogram = model(text_input)\n",
    "\n",
    "        mel_spectogram = mel_spectogram.reshape(audio_input.shape)\n",
    "        loss = loss_fn(mel_spectogram, audio_input)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # some stats\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "def train(model, loss_fn, optimizer, text_dataloader, audio_dataloader, device, epochs):\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        t_losses = train_one_step(model=model,\n",
    "                                  loss_fn=loss_fn,\n",
    "                                  optimizer=optimizer,\n",
    "                                  text_dataloader=text_dataloader,\n",
    "                                  audio_dataloader=audio_dataloader,\n",
    "                                  device=device)\n",
    "\n",
    "        losses.append(t_losses)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 319]), torch.Size([1, 64, 319]))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_dataset[0].shape, music_dataset[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 40, 510]), torch.Size([2, 40, 71]))"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embd = 256\n",
    "# x = torch.randn((2, 40, n_embd)) # batch, max_len, n_embd\n",
    "# fft = FFTBlock(n_embd=n_embd,\n",
    "#                num_heads=2,\n",
    "#                device=device,\n",
    "#                mask=None,\n",
    "#                kernel_size=3,\n",
    "#                stride=1,\n",
    "#                padding=1,\n",
    "#                max_seq_len=40)\n",
    "# fft(x).shape\n",
    "\n",
    "x = torch.randn((2, 40, vocab_size)).type(\n",
    "    torch.float32)  # batch, max_len, n_embd\n",
    "\n",
    "mo = Encoder(max_seq_len=40,\n",
    "             n_embd=n_embd,\n",
    "             device=device,\n",
    "             n_heads=2,\n",
    "             mask=None,\n",
    "             kernel_size=3,\n",
    "             padding=1,\n",
    "             stride=1,\n",
    "             vocab_size=vocab_size)\n",
    "\n",
    "mo(x).shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "N_EMBD = 256\n",
    "N_HEADS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "model = Encoder(max_seq_len=MAX_SEQ_LEN,\n",
    "                n_embd=N_EMBD,\n",
    "                device=device,\n",
    "                n_heads=N_HEADS,\n",
    "                mask=None,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                stride=1,\n",
    "                vocab_size=vocab_size)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:07,  2.61s/it][00:00<?, ?it/s]\n",
      "3it [00:06,  2.19s/it][00:07<01:10,  7.84s/it]\n",
      "3it [00:06,  2.03s/it][00:14<00:56,  7.10s/it]\n",
      "3it [00:05,  1.90s/it][00:20<00:46,  6.64s/it]\n",
      "3it [00:06,  2.15s/it][00:26<00:37,  6.28s/it]\n",
      "3it [00:05,  1.95s/it][00:32<00:31,  6.34s/it]\n",
      "3it [00:06,  2.20s/it][00:38<00:24,  6.18s/it]\n",
      "3it [00:08,  2.73s/it][00:45<00:18,  6.33s/it]\n",
      "3it [00:05,  1.96s/it][00:53<00:13,  6.92s/it]\n",
      "3it [00:05,  1.88s/it][00:59<00:06,  6.60s/it]\n",
      "100%|██████████| 10/10 [01:04<00:00,  6.49s/it]\n"
     ]
    }
   ],
   "source": [
    "losses = train(model=model,\n",
    "               loss_fn=loss_fn,\n",
    "               optimizer=optimizer,\n",
    "               text_dataloader=text_dataloader,\n",
    "               audio_dataloader=audio_dataloader,\n",
    "               device=device,\n",
    "               epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2291ae2bfd0>"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
